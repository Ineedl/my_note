## items.py文件

```python
import scrapy


class MyScrapyItem(scrapy.Item):
    # define the fields for your item here like:
    # name = scrapy.Field()
    pass
```

items.py中，MyScrapyItem即为一个结构，可以在类中使用如下方法添加成员变量

```python
name = scrapy.Field()
```

例如

```python
class MyScrapyItem(scrapy.Item):
    provice = scrapy.Field()
    city_name = scrapy.Field()
    yy_full_name = scrapy.Field()
    first_name = scrapy.Field()
    second_name = scrapy.Field()
    ys = scrapy.Field()
    zc = scrapy.Field()
#MyScrapyItem结构拥有 

#provice 
#city_name 
#yy_full_name 
#first_name 
#second_name 
#ys 
#zc

#6个成员变量
```

* 使用例子

> 结构

```python
class MyScrapyItem(scrapy.Item):
    provice = scrapy.Field()
    city = scrapy.Field()
```

> 爬虫文件中

```python
def parse(self, response):
  provice_name = re.findall("provice", response.text, re.I)
  city_name = re.findall("city", response.text, re.I)
  yield MyScrapyItem(provice = provice_name , city = city_name)		#之后就会交给管道处理
```





## piplines.py文件

```python
class MyScrapyPipeline: 
  
    #默认不会有
    #爬虫项目开始时会调用，常用于初始化数据库操作等
  	def open_spider(self,spider):
        pass
      
    #用户爬虫文件中yield的结构变量会传入并且处理
    def process_item(self, item, spider):
      
      	#return后的item会被info日志打印，不return也不会影响
        return item
      
    #默认不会有
    #爬虫项目结束时会调用，常用于关闭数据库操作等
    def close_spider(self,spider):
        pass
```

* 注意scrapy的管道处理为单线程，而且不允让scrapy并发处理管道
