[toc]

# 损失函数

## 1. **均方误差（MSE, Mean Squared Error）**

用于回归问题，惩罚预测值与真实值之间的平方差。
$$
\mathcal{L}_{\text{MSE}} = \frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2
$$

## 2. **均方根误差（RMSE, Root Mean Squared Error）**

$$
\mathcal{L}_{\text{RMSE}} = \sqrt{\frac{1}{n} \sum_{i=1}^{n} (y_i - \hat{y}_i)^2}
$$

## 3. **平均绝对误差（MAE, Mean Absolute Error）**

$$
\mathcal{L}_{\text{MAE}} = \frac{1}{n} \sum_{i=1}^{n} |y_i - \hat{y}_i|
$$

## 4. **Huber 损失（对异常值更健壮、不容易受干扰）**

$$
\mathcal{L}_{\delta}(y, \hat{y}) =
\begin{cases}
\frac{1}{2}(y - \hat{y})^2 & \text{if } |y - \hat{y}| \leq \delta \\
\delta \cdot \left(|y - \hat{y}| - \frac{1}{2} \delta\right) & \text{otherwise}
\end{cases}
$$

## 5.**对数损失（Log Loss / Binary Cross-Entropy）**

用于二分类问题：
$$
\mathcal{L}_{\text{BCE}} = - \frac{1}{n} \sum_{i=1}^{n} \left[ y_i \log(\hat{y}_i) + (1 - y_i) \log(1 - \hat{y}_i) \right]
$$

## 6. **多类交叉熵损失（Categorical Cross-Entropy）**

用于多分类（one-hot 编码）：
$$
\mathcal{L}_{\text{CCE}} = - \sum_{i=1}^{n} \sum_{k=1}^{K} y_{i,k} \log(\hat{y}_{i,k})
$$

## 7. **KL 散度（Kullback–Leibler Divergence）**

$$
D_{\text{KL}}(P \parallel Q) = \sum_{i} P(i) \log \left( \frac{P(i)}{Q(i)} \right)
$$

## 8. **Hinge 损失（用于 SVM）**

$$
\mathcal{L}_{\text{hinge}} = \sum_{i=1}^{n} \max(0, 1 - y_i \cdot \hat{y}_i)
$$

## 9. **Focal Loss**

用于类别不平衡的分类问题
$$
\mathcal{L}_{\text{focal}} = -\alpha_t (1 - \hat{y}_t)^{\gamma} \log(\hat{y}_t)
$$
其中 
$$
\hat{y}_t 是正确类别的预测概率，\alpha_t 是对应类别的权重，\gamma 是调节因子。
$$

# 激活函数

## 1. **Sigmoid 函数**

将输入压缩到 (0, 1) 区间。
$$
\sigma(x) = \frac{1}{1 + e^{-x}}
$$

- ✅ 优点：输出范围有限，常用于二分类
- ❌ 缺点：容易导致梯度消失（特别是深层网络）

## 2. **Tanh 函数（双曲正切）**

将输入压缩到 (-1, 1) 区间。
$$
\tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}} = 2\sigma(2x) - 1
$$

- ✅ 输出是零中心，更利于收敛
- ❌ 同样存在梯度消失问题

## 3. **ReLU（Rectified Linear Unit）**

最常用的激活函数之一。
$$
\text{ReLU}(x) = \max(0, x)
$$

- ✅ 计算简单，收敛快，缓解梯度消失
- ❌ 不可导于 $x = 0$，且负区间为零，可能导致“神经元死亡”

## 4. **Leaky ReLU**

解决 ReLU 的“神经元死亡”问题，负区间保留较小斜率。
$$
\text{Leaky ReLU}(x) = \begin{cases}
x & \text{if } x > 0 \\
\alpha x & \text{otherwise}
\end{cases}
$$
一般取 $\alpha = 0.01$

## 5. **Parametric ReLU (PReLU)**

Leaky ReLU 的参数化版本，$\alpha$ 是可学习的参数。
$$
\text{PReLU}(x) = \begin{cases}
x & \text{if } x > 0 \\
\alpha x & \text{otherwise}
\end{cases}
$$

## 6. **ELU（Exponential Linear Unit）**

$$
\text{ELU}(x) = \begin{cases}
x & \text{if } x \geq 0 \\
\alpha (e^x - 1) & \text{if } x < 0
\end{cases}
$$

- 输出均值更接近 0，训练更平稳
- 参数 $\alpha > 0$

## 7. **Swish（Google 提出的）**

$$
\text{Swish}(x) = x \cdot \sigma(x) = \frac{x}{1 + e^{-x}}
$$

- ✅ 平滑、非单调，在很多任务中表现优于 ReLU

## 8. **Softmax（多分类输出）**

用于多分类问题的输出层，将向量压缩为概率分布：
$$
\text{Softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^{K} e^{z_j}}
$$

- 输出在 [0,1] 且总和为 1
- 一般**只用在最后一层**