## middlewares.py

```python

from scrapy import signals

from itemadapter import is_item, ItemAdapter

#爬虫中间件
class MyScrapySpiderMiddleware:

  	#scrapy框架会调用该方法返回一个中间件来使用
    #该方法可以使用scrapy中的各种配置(从crawler)来初始化
    @classmethod
    def from_crawler(cls, crawler):
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

   	#当Spider处理Response返回结果之前，process_spider_input会被调用
    #process_spider_input 应该返回None或者抛出一个异常
    #返回None时：
		#Scrapy会继续处理Response，低优先级中间件的process_spider_input方法，直到spider处理该Response
    #抛出异常时：
    #Scrapy将不会调用低优先级中间件的process_spider_input方法，而调用Request的errback方法。errback的输出将会被重新输入到中间件中，使用process_spider_output方法来处理，当errback抛出异常时则调用process_spider_exception来处理
    def process_spider_input(self, response, spider):
        return None

    #当Spider处理Response返回结果时，process_spider_output会被调用
		#process_spider_output方法必须返回包含Request或Item的可迭代对象
    def process_spider_output(self, response, result, spider):
        for i in result:
            yield i

    #当Spider程序或该中间件的process_spider_input方法抛出异常时process_spider_exception会被调用
		#方法必须返回None或者返回一个包含Request或者Item的可迭代对象
    #返回None时：
		#Scrapy继续处理该异常，调用其他Spider Middleware的process_spider_exception方法，
    #直到所有Spider Middleware被调用
		#返回可迭代对象时：
		#调用低优先级中间件的process_spider_output方法
    def process_spider_exception(self, response, exception, spider):
        pass

    #process_start_requests方法以Spider启动的Request为参数被调用，
    #执行的过程类似于process_spider_output，只不过他没有相关联的Response，
    #该方法必须返回一个包含Request的可迭代对象。
    def process_start_requests(self, start_requests, spider):
        for r in start_requests:
            yield r

    #spider文件运行开始经过该中间件时，该方法会被调用
    def spider_opened(self, spider):
        spider.logger.info('Spider opened: %s' % spider.name)

#下载中间件
class MyScrapyDownloaderMiddleware:

  	#scrapy框架会调用该方法返回一个中间件来使用
    #该方法可以使用scrapy中的各种配置(从crawler)来初始化
    @classmethod
    def from_crawler(cls, crawler):
        s = cls()
        crawler.signals.connect(s.spider_opened, signal=signals.spider_opened)
        return s

    #每个交给下载器的request对象都会经过该方法
    #每当scrapy进行一个request请求时，这个方法被调用。
    #通常它可以返回1.None 2.Response对象 3.Request对象 4.抛出IgnoreRequest对象
    def process_request(self, request, spider):
        return None

    #当request请求发出去获得返回并返回响应给引擎的时候这个方法会被调用。
    #它会返回 1.Response对象 2.Request对象 3.抛出IgnoreRequest对象
    def process_response(self, request, response, spider):
        return response

    #当下载处理模块或process_request()抛出一个异常（包括IgnoreRequest异常）时，该方法被调用
		#通常返回None,不然交给下个管道处理
    #可以返回request,response,None
    def process_exception(self, request, exception, spider):
        pass

    #spider文件运行开始经过该中间件时，该方法会被调用
    def spider_opened(self, spider):
        spider.logger.info('Spider opened: %s' % spider.name)

```



## scrapy爬取时常见异常类型

| 错误类型               | 错误名称                                     |
| ---------------------- | -------------------------------------------- |
| defer.TimeoutError (import twisted.internet.error) | 当延迟超时时，默认情况下会引发此错误 |
| TimeoutError (import twisted.internet.error) | 用户超时导致连接失败                         |
| DNSLookupError (import twisted.internet.error) | DNS查找失败                                  |
| ConnectionRefusedError (import twisted.internet.error) | 连接被另一方拒绝                             |
| ConnectionDone (import twisted.internet.error) | 连接干净整洁的关闭了                         |
| ConnectError (import twisted.internet.error) | 连接时发生错误                               |
| ConnectionLost (import twisted.internet.error) | 与另一方的连接以不干净的方式丢失             |
| TCPTimedOutError (import twisted.internet.error) | TCP连接超时                                  |
| ResponseFailed (import twisted.web._newclient) | 由于某种原因没有收到对请求的所有回复         |
| TunnelError (import twisted.internet.error) |	代理无法建立HTTP CONNECT隧道。|
|IOError|I/O相关错误 |

