[toc]

## RunnableWithMessageHistory

ä¸€ä¸ª **é«˜é˜¶å°è£…å™¨**ï¼ŒæŠŠä¸€ä¸ª `Runnable`ï¼ˆé“¾ã€æ¨¡å‹ç­‰ï¼‰åŒ…è£¹èµ·æ¥ï¼Œè‡ªåŠ¨å¤„ç†æ¶ˆæ¯å†å²ã€‚

èƒ½åœ¨æ¯æ¬¡è°ƒç”¨æ—¶ï¼Œæ ¹æ®ä¸€ä¸ª session id è·å–å¯¹åº”å†å²ï¼Œå¹¶è‡ªåŠ¨æ·»åŠ ç”¨æˆ·å’Œ AI æ¶ˆæ¯ã€‚

## BaseChatMessageHistory

è¿™æ˜¯ä¸€ä¸ª **æŠ½è±¡ç±»**ï¼Œå®šä¹‰äº†ç®¡ç†æ¶ˆæ¯å†å²çš„ç»Ÿä¸€æ¥å£ã€‚

æ‰€æœ‰çš„æ¶ˆæ¯å†å²å®ç°éƒ½è¦ç»§æ‰¿å®ƒï¼Œæ¯”å¦‚ä½¿ç”¨å†…å­˜ã€Redisã€DynamoDBç­‰å­˜å‚¨æ–¹å¼ã€‚

## å­˜å‚¨åœ¨å†…å­˜

### ChatMessageHIstory

`BaseChatMessageHistory` çš„ä¸€ä¸ª **å†…å­˜ä¸­é»˜è®¤å®ç°**ã€‚

é€‚ç”¨äºç®€å•çš„å¯¹è¯åœºæ™¯æˆ–å¼€å‘é˜¶æ®µï¼Œæ— éœ€æŒä¹…åŒ–ã€‚

`ä½¿ç”¨ä¾‹å­`

```python
import asyncio

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_deepseek import ChatDeepSeek
from pydantic import  SecretStr
from langchain_community.chat_message_histories import  ChatMessageHistory
from langchain_core.chat_history import  BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

llm = ChatDeepSeek(
    model="deepseek-chat",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    api_key = SecretStr("sk-xxxxxxxxxx")
)

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You're an assistant who's good at {ability}. Respond in 20 words or fewer"
        ),
        MessagesPlaceholder(variable_name="history"),
        ("human","{input}"),
    ]
)

store={}

runnable = prompt | llm

def get_session_history(session_id:str) -> BaseChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# è®©runnableå…·æœ‰å†å²æ¶ˆæ¯åŠŸèƒ½
with_message_history = RunnableWithMessageHistory(
    runnable,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history",
)

# è°ƒç”¨æ–°çš„runnable
response = with_message_history.invoke(
    input={"ability":"math","input":"æ­£å¼¦æ˜¯ä»€ä¹ˆæ„æ€"},
    config={"configurable":{"session_id":"abc123"}},
)

print(response)

print("------------------------")

response = with_message_history.invoke(
    input={"ability":"math","input":"ä»€ä¹ˆ? å†è¯´ä¸€æ¬¡"},
    config={"configurable":{"session_id":"abc123"}},
)

print(response)

print("------------------------")
#sessionä¸åŒ ä¸å†è®°å¾—
response = with_message_history.invoke(
    input={"ability":"math","input":"ä»€ä¹ˆ? å†è¯´ä¸€æ¬¡"},
    config={"configurable":{"session_id":"abc1234"}},
)

print(response)
```

`ä½¿ç”¨ConfigurableFieldSpecæ¥è§„èŒƒåŒ–å†å²å‚æ•°`

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_deepseek import ChatDeepSeek
from pydantic import  SecretStr
from langchain_community.chat_message_histories import  ChatMessageHistory
from langchain_core.chat_history import  BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

from langchain_core.runnables.history import  ConfigurableFieldSpec

llm = ChatDeepSeek(
    model="deepseek-chat",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    api_key = SecretStr("sk-3f5705af4ce346689eec074d9e81e11d")
)

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You're an assistant who's good at {ability}. Respond in 20 words or fewer"
        ),
        MessagesPlaceholder(variable_name="history"),
        ("human","{input}"),
    ]
)

store={}

runnable = prompt | llm

def get_session_history(user_id:str,conversation_id:str) -> BaseChatMessageHistory:
    if (user_id,conversation_id) not in store:
        store[(user_id,conversation_id)] = ChatMessageHistory()
    return store[(user_id,conversation_id)]

# è®©runnableå…·æœ‰å†å²æ¶ˆæ¯åŠŸèƒ½
with_message_history = RunnableWithMessageHistory(
    runnable,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history",
    history_factory_config=[
        ConfigurableFieldSpec(
            id="user_id",
            annotation=str,
            name="User ID",
            description="ç”¨æˆ·å”¯ä¸€æ ‡è¯†",
            default="",
            is_shared=True,
        ),
        ConfigurableFieldSpec(
            id="conversation_id",
            annotation=str,
            name="Conversation ID",
            description="å¯¹è¯çš„å”¯ä¸€æ ‡è¯†",
            default="",
            is_shared=True,
        )
    ]
)

# è°ƒç”¨æ–°çš„runnable
response = with_message_history.invoke(
    input={"ability":"math","input":"æ­£å¼¦æ˜¯ä»€ä¹ˆæ„æ€"},
    config={"configurable":{"user_id":"cjh","conversation_id":"1"}},
)

print(response)


print("----------------")

response = with_message_history.invoke(
    input={"ability":"math","input":"å†è¯´ä¸€æ¬¡?"},
    config={"configurable":{"user_id":"cjh","conversation_id":"1"}},
)

print(response)

print("----------------")

response = with_message_history.invoke(
    input={"ability":"math","input":"å†è¯´ä¸€æ¬¡?"},
    config={"configurable":{"user_id":"cjh","conversation_id":"2"}},
)

print(response)
```

### RedisChatMessageHistory

é›†æˆredisçš„å†å²æ¶ˆæ¯

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_deepseek import ChatDeepSeek
from pydantic import  SecretStr
from langchain_community.chat_message_histories import ChatMessageHistory, RedisChatMessageHistory
from langchain_core.chat_history import  BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

from langchain_core.runnables.history import  ConfigurableFieldSpec

llm = ChatDeepSeek(
    model="deepseek-chat",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    api_key = SecretStr("sk-3f5705af4ce346689eec074d9e81e11d")
)

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You're an assistant who's good at {ability}. Respond in 20 words or fewer"
        ),
        MessagesPlaceholder(variable_name="history"),
        ("human","{input}"),
    ]
)

REDIS_URL = "redis://127.0.0.1:6379/1"

runnable = prompt | llm

def get_session_history(session_id:str) -> BaseChatMessageHistory:
    return RedisChatMessageHistory(session_id,REDIS_URL)

# è®©runnableå…·æœ‰å†å²æ¶ˆæ¯åŠŸèƒ½
with_message_history = RunnableWithMessageHistory(
    runnable,
    get_session_history,
    input_messages_key="input",
    history_messages_key="history",
)

# è°ƒç”¨æ–°çš„runnable
response = with_message_history.invoke(
    input={"ability":"math","input":"æ­£å¼¦æ˜¯ä»€ä¹ˆæ„æ€"},
    config={"configurable":{"session_id":"abc1234"}},
)

print(response)


print("----------------")

response = with_message_history.invoke(
    input={"ability":"math","input":"å†è¯´ä¸€æ¬¡?"},
    config={"configurable": {"session_id": "abc1234"}},
)

print(response)

print("----------------")

response = with_message_history.invoke(
    input={"ability":"math","input":"å†è¯´ä¸€æ¬¡?"},
    config={"configurable": {"session_id": "abc12345"}},
)

print(response)
```

## æ¶ˆæ¯è£å‰ª

### RunnablePassthrough

ä½¿ç”¨RunnablePassthroughå¯ä»¥è£å‰ªå’Œå‹ç¼©å†å²æ¶ˆæ¯ï¼Œé¿å…èŠå¤©è¿‡é•¿å¯¼è‡´å†å²æ¶ˆæ¯è¿‡å¤§ï¼Œtokenæ¶ˆè€—è¿‡å¤šã€‚

#### è£å‰ª

```python
import asyncio

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_deepseek import ChatDeepSeek
from pydantic import  SecretStr
from langchain_community.chat_message_histories import  ChatMessageHistory
from langchain_core.chat_history import  BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

llm = ChatDeepSeek(
    model="deepseek-chat",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    api_key = SecretStr("sk-xxxxxxxxxx")
)

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "You're an assistant who's good at {ability}. Respond in 20 words or fewer"
        ),
        MessagesPlaceholder(variable_name="history"),
        ("human","{input}"),
    ]
)

runnable = prompt | llm

temp_chat_history = ChatMessageHistory()
temp_chat_history.add_user_message("æ­£å¼¦æ˜¯ä»€ä¹ˆ?")
temp_chat_history.add_ai_message("å¯¹è¾¹æ¯”æ–œè¾¹")
temp_chat_history.add_user_message("ä½™å¼¦æ˜¯ä»€ä¹ˆ?")
temp_chat_history.add_ai_message("é‚»è¾¹æ¯”æ–œè¾¹")
tmp_chat_history.messages
  
# è®©runnableå…·æœ‰å†å²æ¶ˆæ¯åŠŸèƒ½
with_message_history = RunnableWithMessageHistory(
    runnable,
    lambda session_id: temp_chat_history,		#æ— è§†ä¼ å…¥çš„session_id
    input_messages_key="input",
    history_messages_key="history",
)


#é™åˆ¶å†å²è®°å½•ä¸º1
def trim_message(chain_input):
  stored_messages = temp_chat_history.message
  if len(stored_messages) <= 1:
    return false
  temp_chat_history.clear()
  for message in stored_messages[-1:]:
    temp_chat_history.add_message(message)
  return True

chain_with_trimming = RunnablePassthrough.assign(messages_trimmed=trim_message) | with_message_history


# è°ƒç”¨æ–°çš„runnable
print("------------------------")
# ä¸å†è®°å¾—é—®è¿‡æ­£å¼¦
response = chain_with_trimming.invoke(
    input={"ability":"math","input":"æˆ‘çš„ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯ä»€ä¹ˆ?"},
    config={"configurable":{"session_id":"abc123"}},
)

```

#### å‹ç¼©

```python
import asyncio

from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables import RunnablePassthrough
from langchain_deepseek import ChatDeepSeek
from pydantic import SecretStr
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.chat_history import BaseChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

llm = ChatDeepSeek(
    model="deepseek-chat",
    temperature=0,
    max_tokens=None,
    timeout=None,
    max_retries=2,
    api_key=SecretStr("sk-xxxxxxxxxx")
)

prompt = ChatPromptTemplate.from_messages(
    [
        (
            "system",
            "ä½ æ˜¯ä¸€ä¸ªä¹äºåŠ©äººçš„åŠ©æ‰‹"
        ),
        MessagesPlaceholder(variable_name="chat_history"),
        ("human", "{input}"),
    ]
)

runnable = prompt | llm

temp_chat_history = ChatMessageHistory()
temp_chat_history.add_user_message("æ­£å¼¦æ˜¯ä»€ä¹ˆ?")
temp_chat_history.add_ai_message("å¯¹è¾¹æ¯”æ–œè¾¹")
temp_chat_history.add_user_message("ä½™å¼¦æ˜¯ä»€ä¹ˆ?")
temp_chat_history.add_ai_message("é‚»è¾¹æ¯”æ–œè¾¹")

# è®©runnableå…·æœ‰å†å²æ¶ˆæ¯åŠŸèƒ½
with_message_history = RunnableWithMessageHistory(
    runnable,
    lambda session_id: temp_chat_history,
    input_messages_key = "input",
    history_messages_key = "chat_history",
)

# å‹ç¼©æ¶ˆæ¯
def summarize_messages(chain_input):
    stored_messages = temp_chat_history.messages
    if len(stored_messages) == 0:
        return False
    summarization_prompt = ChatPromptTemplate.from_messages(
        [
            MessagesPlaceholder(variable_name="chat_history"),
            (
                "user",
                "å°†ä¸Šè¿°èŠå¤©æ¶ˆæ¯æµ“ç¼©æˆä¸€æ¡æ‘˜è¦æ¶ˆæ¯ï¼Œå°½å¯èƒ½åŒ…å«å¤šä¸ªå…·ä½“ç»†èŠ‚"
            )
        ]
    )
    summarization_chain = summarization_prompt | llm
    summary_message = summarization_chain.invoke({"chat_history": stored_messages})
    temp_chat_history.clear()
    temp_chat_history.add_message(summary_message)
    return True


chain_with_summarization = RunnablePassthrough.assign(message_summarized=summarize_messages) | with_message_history

# è°ƒç”¨æ–°çš„runnable
print("------------------------")
# ä¸å†è®°å¾—é—®è¿‡æ­£å¼¦
response = chain_with_summarization.invoke(
    input={"input": "æˆ‘çš„ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯ä»€ä¹ˆ?"},
    config={"configurable": {"session_id": "abc123"}},
)
print(response)
print(temp_chat_history.messages)


#è¾“å‡º
content='ä½ çš„ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯å…³äº**ç›´è§’ä¸‰è§’å½¢ä¸­çš„æ­£å¼¦ï¼ˆsinï¼‰å’Œä½™å¼¦ï¼ˆcosï¼‰çš„å®šä¹‰**ï¼Œå…·ä½“å†…å®¹æ˜¯ï¼š  \n\n> **æ­£å¼¦ï¼ˆsinï¼‰**å®šä¹‰ä¸º**å¯¹è¾¹ä¸æ–œè¾¹çš„æ¯”å€¼**ï¼Œè€Œ**ä½™å¼¦ï¼ˆcosï¼‰**å®šä¹‰ä¸º**é‚»è¾¹ä¸æ–œè¾¹çš„æ¯”å€¼**ã€‚ä¸¤è€…å‡ä¸ºä¸‰è§’å‡½æ•°ï¼Œç”¨äºæè¿°è§’åº¦ä¸è¾¹é•¿å…³ç³»ã€‚  \n\néœ€è¦æˆ‘è¿›ä¸€æ­¥è§£é‡Šæˆ–è¡¥å……ç›¸å…³å†…å®¹å—ï¼Ÿ ğŸ˜Š' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 80, 'prompt_tokens': 69, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 69}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_3d5141a69a_prod0225', 'id': '05d423f9-4157-4709-acef-c57aca7306c3', 'finish_reason': 'stop', 'logprobs': None} id='run-f64932d5-664e-44ad-9902-76a05d27c2d1-0' usage_metadata={'input_tokens': 69, 'output_tokens': 80, 'total_tokens': 149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}
[AIMessage(content='**æ‘˜è¦æ¶ˆæ¯**ï¼š  \nåœ¨ç›´è§’ä¸‰è§’å½¢ä¸­ï¼Œ**æ­£å¼¦ï¼ˆsinï¼‰**å®šä¹‰ä¸º**å¯¹è¾¹ä¸æ–œè¾¹çš„æ¯”å€¼**ï¼Œè€Œ**ä½™å¼¦ï¼ˆcosï¼‰**å®šä¹‰ä¸º**é‚»è¾¹ä¸æ–œè¾¹çš„æ¯”å€¼**ã€‚ä¸¤è€…å‡ä¸ºä¸‰è§’å‡½æ•°ï¼Œç”¨äºæè¿°è§’åº¦ä¸è¾¹é•¿å…³ç³»ã€‚', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 54, 'prompt_tokens': 40, 'total_tokens': 94, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 40}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_3d5141a69a_prod0225', 'id': '43a1bbd5-8e23-48e6-8a4f-c84f48a2b203', 'finish_reason': 'stop', 'logprobs': None}, id='run-cc9205fb-088d-4398-ae7a-7589b53205a6-0', usage_metadata={'input_tokens': 40, 'output_tokens': 54, 'total_tokens': 94, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}}), HumanMessage(content='æˆ‘çš„ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯ä»€ä¹ˆ?', additional_kwargs={}, response_metadata={}), AIMessage(content='ä½ çš„ç¬¬ä¸€ä¸ªé—®é¢˜æ˜¯å…³äº**ç›´è§’ä¸‰è§’å½¢ä¸­çš„æ­£å¼¦ï¼ˆsinï¼‰å’Œä½™å¼¦ï¼ˆcosï¼‰çš„å®šä¹‰**ï¼Œå…·ä½“å†…å®¹æ˜¯ï¼š  \n\n> **æ­£å¼¦ï¼ˆsinï¼‰**å®šä¹‰ä¸º**å¯¹è¾¹ä¸æ–œè¾¹çš„æ¯”å€¼**ï¼Œè€Œ**ä½™å¼¦ï¼ˆcosï¼‰**å®šä¹‰ä¸º**é‚»è¾¹ä¸æ–œè¾¹çš„æ¯”å€¼**ã€‚ä¸¤è€…å‡ä¸ºä¸‰è§’å‡½æ•°ï¼Œç”¨äºæè¿°è§’åº¦ä¸è¾¹é•¿å…³ç³»ã€‚  \n\néœ€è¦æˆ‘è¿›ä¸€æ­¥è§£é‡Šæˆ–è¡¥å……ç›¸å…³å†…å®¹å—ï¼Ÿ ğŸ˜Š', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 80, 'prompt_tokens': 69, 'total_tokens': 149, 'completion_tokens_details': None, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}, 'prompt_cache_hit_tokens': 0, 'prompt_cache_miss_tokens': 69}, 'model_name': 'deepseek-chat', 'system_fingerprint': 'fp_3d5141a69a_prod0225', 'id': '05d423f9-4157-4709-acef-c57aca7306c3', 'finish_reason': 'stop', 'logprobs': None}, id='run-f64932d5-664e-44ad-9902-76a05d27c2d1-0', usage_metadata={'input_tokens': 69, 'output_tokens': 80, 'total_tokens': 149, 'input_token_details': {'cache_read': 0}, 'output_token_details': {}})]

```

